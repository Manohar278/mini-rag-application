{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "37d5d357-4030-4dd4-ac4b-484f0ed0b3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Celia's Shrine\n",
      "by David Gardiner\n",
      "This story may be reproduced in whole or in part for any non-commercial\n",
      "purpose on condition\n",
      "that authorship is acknowledged and credited. The copyright remains the\n",
      "property of the author.\n",
      "I'm glad you like the bungalow. I would like it to go to a happy young couple\n",
      "like you. We were always very happy here. Well, as happy as anybody ever\n",
      "is... you know what I mean. Why don't you sit down and I'll make the two of\n",
      "you a cup of tea?\n",
      "This was out in the country when \n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "reader = PdfReader(\"Celia's Shrine.pdf\")\n",
    "\n",
    "text = \"\"\n",
    "for page in reader.pages:\n",
    "    page_text = page.extract_text()\n",
    "    if page_text:  # guard against None\n",
    "        text += page_text\n",
    "\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f450ebc5-742d-4044-abfd-5516f8eeba33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "words=text.split()\n",
    "chunks=[]\n",
    "chunk_size=1000\n",
    "overlap=150\n",
    "step=chunk_size-overlap\n",
    "\n",
    "for start in range(0,len(words),step):\n",
    "    end=start+chunk_size\n",
    "    chunk_words=words[start:end]\n",
    "    chunk_text=\"\".join(chunk_words)\n",
    "    chunks.append(chunk_text)\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "190323dd-a719-46a7-976a-15002f0ece7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 241.53it/s, Materializing param=pooler.dense.weight]                             \n",
      "BertModel LOAD REPORT from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0280197  -0.056583    0.06115209 ... -0.00698476  0.04047382\n",
      "  -0.01063019]\n",
      " [-0.02640468 -0.01661091  0.03435023 ... -0.09219253  0.04199612\n",
      "   0.03628125]\n",
      " [-0.00504866 -0.06904926  0.08129273 ... -0.01680476  0.05890396\n",
      "  -0.0038166 ]]\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings=model.encode(chunks)\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2121a957-3139-44cf-ad16-e37e19f2ea51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import pinecone\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "615ad14f-ea4a-4e73-95fd-36d953d82592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone ✅\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone\n",
    "PINECONE_API_KEY = \"pcsk_7N8TDx_T7CDM66rqR9VvwPRVHfCZd1iJejAdNkA1VStAXXRCcnZeRBcWMibb5pHyXCBfSk\"\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "print(\"Connected to Pinecone ✅\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a9f5640-c558-4fab-855a-a3a5064a55fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created or already exists ✅\n"
     ]
    }
   ],
   "source": [
    "from pinecone import Pinecone, ServerlessSpec\n",
    "\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
    "\n",
    "index_name = \"mini-rag\"\n",
    "\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    pc.create_index(\n",
    "        name=index_name,\n",
    "        dimension=384,\n",
    "        metric=\"cosine\",\n",
    "        spec=ServerlessSpec(\n",
    "            cloud=\"aws\",\n",
    "            region=\"us-east-1\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "index = pc.Index(index_name)\n",
    "print(\"Index created or already exists ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "23b8d0c4-8f00-48fc-bed5-7885eab55b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []   # empty list to store data\n",
    "\n",
    "for i in range(len(chunks)):\n",
    "    chunk_id = \"chunk-\" + str(i)      # unique ID (string)\n",
    "    embedding = embeddings[i].tolist()  # convert to list\n",
    "    metadata = {\n",
    "        \"text\": chunks[i]             # original chunk text\n",
    "    }\n",
    "\n",
    "    vectors.append((chunk_id, embedding, metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "18b2b279-1758-41dd-801a-62d6099eaabc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpsertResponse(upserted_count=3, _response_info={'raw_headers': {'date': 'Tue, 03 Feb 2026 15:32:11 GMT', 'content-type': 'application/json', 'content-length': '19', 'connection': 'keep-alive', 'x-pinecone-request-lsn': '3', 'x-pinecone-request-logical-size': '14699', 'x-pinecone-request-latency-ms': '540', 'x-pinecone-request-id': '136333665311235913', 'x-envoy-upstream-service-time': '165', 'x-pinecone-response-duration-ms': '541', 'grpc-status': '0', 'server': 'envoy'}})"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.upsert(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8a089818-2372-40a1-9001-ad4c91f9bf8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_response_info': {'raw_headers': {'connection': 'keep-alive',\n",
      "                                    'content-length': '181',\n",
      "                                    'content-type': 'application/json',\n",
      "                                    'date': 'Tue, 03 Feb 2026 15:32:12 GMT',\n",
      "                                    'grpc-status': '0',\n",
      "                                    'server': 'envoy',\n",
      "                                    'x-envoy-upstream-service-time': '4',\n",
      "                                    'x-pinecone-request-id': '5274182028047683469',\n",
      "                                    'x-pinecone-request-latency-ms': '6',\n",
      "                                    'x-pinecone-response-duration-ms': '8'}},\n",
      " 'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'memoryFullness': 0.0,\n",
      " 'metric': 'cosine',\n",
      " 'namespaces': {'__default__': {'vector_count': 3}},\n",
      " 'storageFullness': 0.0,\n",
      " 'total_vector_count': 3,\n",
      " 'vector_type': 'dense'}\n"
     ]
    }
   ],
   "source": [
    "print(index.describe_index_stats())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b98d6cc-ed38-4386-a8ba-4a1875ec616f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_text = \"What is this document about?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00cc53b3-d1bc-4ff8-b0fe-27a7e298f367",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embedding = model.encode(query_text).tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f473a987-3277-4b34-8566-f87c0db94f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = index.query(\n",
    "    vector=question_embedding,\n",
    "    top_k=5,\n",
    "    include_metadata=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9da07d6c-f47e-43da-9a37-5a9b4dca10a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1:\n",
      "goon.IthoughttheleastwecoulddowasletthemuseCharlie'sroomforafewdayswhiletheysortedthemselvesout.Iwantedtophonethelocalhospitalandputthemonstandby,butthegirlwouldn'thaveit.Saidthatshehadtogivebirthontheearth,facingacertaindirection,whilesomekindofincantationswerereadout....allkindsofstufflikethat.But\n",
      "----------------------------------------\n",
      "Chunk 2:\n",
      "Celia'sShrinebyDavidGardinerThisstorymaybereproducedinwholeorinpartforanynon-commercialpurposeonconditionthatauthorshipisacknowledgedandcredited.Thecopyrightremainsthepropertyoftheauthor.I'mgladyoulikethebungalow.Iwouldlikeittogotoahappyyoungcouplelikeyou.Wewerealwaysveryhappyhere.Well,ashappyasanyb\n",
      "----------------------------------------\n",
      "Chunk 3:\n",
      "pouredhimastiffdrinkandtookitouttohim.Hedowneditinone.Talkingaboutdrinks,Ithinkthekettle'sboiling.I'lljustgoandmakethetea.Thereisn'tmuchmoretotellyouanyway.Theending?Ohthereisn'tonereally.Theirfriendsintheothervanloadedupallthestuffoutofthebroken-downoneandtookitaway.Apparentlytheyweregoingtosellita\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, match in enumerate(results[\"matches\"]):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(match[\"metadata\"][\"text\"][:300])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "64571932-04f5-4720-a329-531b86692d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_chunks = []\n",
    "\n",
    "for match in results[\"matches\"]:\n",
    "    context_chunks.append(match[\"metadata\"][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7f8ac050-b3a4-4916-8719-edd75b2f4e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\\n\\n\".join(context_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ca04ee6e-0399-43c3-bce1-96f95d326df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not context_chunks:\n",
    "    answer = \"I don't know. The document does not contain this information.\"\n",
    "else:\n",
    "    answer = \"Based on the document:\\n\\n\"\n",
    "    for i, chunk in enumerate(context_chunks):\n",
    "        answer += f\"[{i+1}] {chunk[:200]}...\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43077070-f3b3-40b2-9085-ae0e147b0138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 105/105 [00:00<00:00, 262.05it/s, Materializing param=classifier.weight]                                    \n",
      "BertForSequenceClassification LOAD REPORT from: cross-encoder/ms-marco-MiniLM-L-6-v2\n",
      "Key                          | Status     |  | \n",
      "-----------------------------+------------+--+-\n",
      "bert.embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "193f78e3-e80b-4490-8a34-4fb267ccd836",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "\n",
    "for match in results[\"matches\"]:\n",
    "    chunk_text = match[\"metadata\"][\"text\"]\n",
    "    pairs.append((query_text, chunk_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "73a2c075-af00-4470-b5d5-aa00ac12d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = reranker.predict(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "81c13ff7-4810-4854-b7b0-b9b6b402ec44",
   "metadata": {},
   "outputs": [],
   "source": [
    "reranked = list(zip(scores, results[\"matches\"]))\n",
    "reranked.sort(key=lambda x: x[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ce3da649-9950-4757-b120-04e6e9a6a904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank 1 | Score: -8.7177\n",
      "pouredhimastiffdrinkandtookitouttohim.Hedowneditinone.Talkingaboutdrinks,Ithinkthekettle'sboiling.I'lljustgoandmakethetea.Thereisn'tmuchmoretotellyouanyway.Theending?Ohthereisn'tonereally.Theirfriendsintheothervanloadedupallthestuffoutofthebroken-downoneandtookitaway.Apparentlytheyweregoingtosellita\n",
      "----------------------------------------\n",
      "Rank 2 | Score: -8.7716\n",
      "Celia'sShrinebyDavidGardinerThisstorymaybereproducedinwholeorinpartforanynon-commercialpurposeonconditionthatauthorshipisacknowledgedandcredited.Thecopyrightremainsthepropertyoftheauthor.I'mgladyoulikethebungalow.Iwouldlikeittogotoahappyyoungcouplelikeyou.Wewerealwaysveryhappyhere.Well,ashappyasanyb\n",
      "----------------------------------------\n",
      "Rank 3 | Score: -8.8928\n",
      "goon.IthoughttheleastwecoulddowasletthemuseCharlie'sroomforafewdayswhiletheysortedthemselvesout.Iwantedtophonethelocalhospitalandputthemonstandby,butthegirlwouldn'thaveit.Saidthatshehadtogivebirthontheearth,facingacertaindirection,whilesomekindofincantationswerereadout....allkindsofstufflikethat.But\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i, (score, match) in enumerate(reranked):\n",
    "    print(f\"Rank {i+1} | Score: {score:.4f}\")\n",
    "    print(match[\"metadata\"][\"text\"][:300])\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e3709b94-510e-4737-b9fc-cf1e181b429c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 76/76 [00:00<00:00, 285.32it/s, Materializing param=transformer.wte.weight]            \n",
      "GPT2LMHeadModel LOAD REPORT from: distilgpt2\n",
      "Key                                        | Status     |  | \n",
      "-------------------------------------------+------------+--+-\n",
      "transformer.h.{0, 1, 2, 3, 4, 5}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"distilgpt2\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0b4cf250-6ae5-49f6-8ff6-28740c369ba0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=20) and `max_length`(=50) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello,my name is my first name,my password is my first name,my username is my second name,my password'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Hello,my name is\",max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11eb9123-1d48-41a6-97c3-ff3228273623",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 282/282 [00:01<00:00, 236.41it/s, Materializing param=shared.weight]                                                       \n",
      "The tied weights mapping and config for this model specifies to tie shared.weight to lm_head.weight, but both are present in the checkpoints, so we will NOT tie them. You should update the config with `tie_word_embeddings=False` to silence this warning\n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AfmoeForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'CwmForCausalLM', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'Glm4MoeLiteForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'Jais2ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxM2ForCausalLM', 'MinistralForCausalLM', 'Ministral3ForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NanoChatForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'SolarOpenForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "llm = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"google/flan-t5-base\",\n",
    "    max_new_tokens=256\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "09a854ee-1b10-4a99-ba73-3dbc706e7d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_chunks=reranked[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b7c49ec1-4d26-447a-9d8b-af04cff5dab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = \"\"\n",
    "\n",
    "for i, match in enumerate(results[\"matches\"]):\n",
    "    if match.metadata and \"text\" in match.metadata:\n",
    "        context += f\"[{i+1}] {match.metadata['text']}\\n\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5bd4c83-a748-4253-ae0b-3732da4d6b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if context.strip() == \"\":\n",
    "    final_answer = \"I don't know. The document does not contain this information.\"\n",
    "else:\n",
    "    final_answer = \"Answer based on the document:\\n\\n\"\n",
    "    final_answer += context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d102d70f-745c-4437-95dd-05a90b510910",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
